{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-09T04:13:57.737259Z",
     "start_time": "2024-09-09T04:13:57.729257Z"
    }
   },
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "print(\"hello\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T20:39:43.864230Z",
     "start_time": "2024-09-05T20:39:40.195666Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install moviepy",
   "id": "7e63ccd015da720f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting moviepy\n",
      "  Using cached moviepy-1.0.3.tar.gz (388 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting decorator<5.0,>=4.0.2 (from moviepy)\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\programdata\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages (from moviepy) (4.65.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\programdata\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages (from moviepy) (2.28.2)\n",
      "Collecting proglog<=1.0.0 (from moviepy)\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\programdata\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages (from moviepy) (1.24.3)\n",
      "Collecting imageio<3.0,>=2.5 (from moviepy)\n",
      "  Downloading imageio-2.35.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\n",
      "  Using cached imageio_ffmpeg-0.5.1-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\programdata\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (60.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n",
      "Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Downloading imageio-2.35.1-py3-none-any.whl (315 kB)\n",
      "Using cached imageio_ffmpeg-0.5.1-py3-none-win_amd64.whl (22.6 MB)\n",
      "Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Building wheels for collected packages: moviepy\n",
      "  Building wheel for moviepy (setup.py): started\n",
      "  Building wheel for moviepy (setup.py): finished with status 'done'\n",
      "  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110766 sha256=a42bff5882b6966aa2ee4828505513363a80c9e0f68646473ae7dd2a6a5816b4\n",
      "  Stored in directory: c:\\users\\rodol\\appdata\\local\\pip\\cache\\wheels\\e4\\a4\\db\\0368d3a04033da662e13926594b3a8cf1aa4ffeefe570cfac1\n",
      "Successfully built moviepy\n",
      "Installing collected packages: imageio_ffmpeg, imageio, decorator, proglog, moviepy\n",
      "Successfully installed decorator-4.4.2 imageio-2.35.1 imageio_ffmpeg-0.5.1 moviepy-1.0.3 proglog-0.1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts imageio_download_bin.exe and imageio_remove_bin.exe are installed in 'C:\\Users\\rodol\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:53:52.557667Z",
     "start_time": "2024-09-06T17:53:51.205027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mmaction.utils import register_all_modules\n",
    "register_all_modules(init_default_scope=True)"
   ],
   "id": "2080fff8d9ba046e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import mmcv\n",
    "import decord\n",
    "import numpy as np\n",
    "from mmcv.transforms import TRANSFORMS, BaseTransform, to_tensor\n",
    "from mmaction.structures import ActionDataSample\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoInit(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        container = decord.VideoReader(results['filename'])\n",
    "        results['total_frames'] = len(container)\n",
    "        results['video_reader'] = container\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoSample(BaseTransform):\n",
    "    def __init__(self, clip_len, num_clips, test_mode=False):\n",
    "        self.clip_len = clip_len\n",
    "        self.num_clips = num_clips\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def transform(self, results):\n",
    "        total_frames = results['total_frames']\n",
    "        interval = total_frames // self.clip_len\n",
    "\n",
    "        if self.test_mode:\n",
    "            # Make the sampling during testing deterministic\n",
    "            np.random.seed(42)\n",
    "\n",
    "        inds_of_all_clips = []\n",
    "        for i in range(self.num_clips):\n",
    "            bids = np.arange(self.clip_len) * interval\n",
    "            offset = np.random.randint(interval, size=bids.shape)\n",
    "            inds = bids + offset\n",
    "            inds_of_all_clips.append(inds)\n",
    "\n",
    "        results['frame_inds'] = np.concatenate(inds_of_all_clips)\n",
    "        results['clip_len'] = self.clip_len\n",
    "        results['num_clips'] = self.num_clips\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoDecode(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        frame_inds = results['frame_inds']\n",
    "        container = results['video_reader']\n",
    "\n",
    "        imgs = container.get_batch(frame_inds).asnumpy()\n",
    "        imgs = list(imgs)\n",
    "\n",
    "        results['video_reader'] = None\n",
    "        del container\n",
    "\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoResize(BaseTransform):\n",
    "    def __init__(self, r_size):\n",
    "        self.r_size = (np.inf, r_size)\n",
    "\n",
    "    def transform(self, results):\n",
    "        img_h, img_w = results['img_shape']\n",
    "        new_w, new_h = mmcv.rescale_size((img_w, img_h), self.r_size)\n",
    "\n",
    "        imgs = [mmcv.imresize(img, (new_w, new_h))\n",
    "                for img in results['imgs']]\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoCrop(BaseTransform):\n",
    "    def __init__(self, c_size):\n",
    "        self.c_size = c_size\n",
    "\n",
    "    def transform(self, results):\n",
    "        img_h, img_w = results['img_shape']\n",
    "        center_x, center_y = img_w // 2, img_h // 2\n",
    "        x1, x2 = center_x - self.c_size // 2, center_x + self.c_size // 2\n",
    "        y1, y2 = center_y - self.c_size // 2, center_y + self.c_size // 2\n",
    "        imgs = [img[y1:y2, x1:x2] for img in results['imgs']]\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoFormat(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        num_clips = results['num_clips']\n",
    "        clip_len = results['clip_len']\n",
    "        imgs = results['imgs']\n",
    "\n",
    "        # [num_clips*clip_len, H, W, C]\n",
    "        imgs = np.array(imgs)\n",
    "        # [num_clips, clip_len, H, W, C]\n",
    "        imgs = imgs.reshape((num_clips, clip_len) + imgs.shape[1:])\n",
    "        # [num_clips, C, clip_len, H, W]\n",
    "        imgs = imgs.transpose(0, 4, 1, 2, 3)\n",
    "\n",
    "        results['imgs'] = imgs\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoPack(BaseTransform):\n",
    "    def __init__(self, meta_keys=('img_shape', 'num_clips', 'clip_len')):\n",
    "        self.meta_keys = meta_keys\n",
    "\n",
    "    def transform(self, results):\n",
    "        packed_results = dict()\n",
    "        inputs = to_tensor(results['imgs'])\n",
    "        data_sample = ActionDataSample()\n",
    "        data_sample.set_gt_label(results['label'])\n",
    "        metainfo = {k: results[k] for k in self.meta_keys if k in results}\n",
    "        data_sample.set_metainfo(metainfo)\n",
    "        packed_results['inputs'] = inputs\n",
    "        packed_results['data_samples'] = data_sample\n",
    "         return packed_results"
   ],
   "id": "81a20d62047616df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:53:54.856823Z",
     "start_time": "2024-09-06T17:53:54.642804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@TRANSFORMS.register_module()\n",
    "class VideoSample(BaseTransform):\n",
    "    def __init__(self, clip_len, num_clips, test_mode=False):\n",
    "        self.clip_len = clip_len\n",
    "        self.num_clips = num_clips\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def transform(self, results):\n",
    "        total_frames = results['total_frames']\n",
    "        interval = total_frames // self.clip_len\n",
    "\n",
    "        if self.test_mode:\n",
    "            # Make the sampling during testing deterministic\n",
    "            np.random.seed(42)\n",
    "\n",
    "        inds_of_all_clips = []\n",
    "        for i in range(self.num_clips):\n",
    "            bids = np.arange(self.clip_len) * interval\n",
    "            offset = np.random.randint(interval, size=bids.shape)\n",
    "            inds = bids + offset\n",
    "            inds_of_all_clips.append(inds)\n",
    "\n",
    "        results['frame_inds'] = np.concatenate(inds_of_all_clips)\n",
    "        results['clip_len'] = self.clip_len\n",
    "        results['num_clips'] = self.num_clips\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoDecode(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        frame_inds = results['frame_inds']\n",
    "        container = results['video_reader']\n",
    "\n",
    "        imgs = container.get_batch(frame_inds).asnumpy()\n",
    "        imgs = list(imgs)\n",
    "\n",
    "        results['video_reader'] = None\n",
    "        del container\n",
    "\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoResize(BaseTransform):\n",
    "    def __init__(self, r_size):\n",
    "        self.r_size = (np.inf, r_size)\n",
    "\n",
    "    def transform(self, results):\n",
    "        img_h, img_w = results['img_shape']\n",
    "        new_w, new_h = mmcv.rescale_size((img_w, img_h), self.r_size)\n",
    "\n",
    "        imgs = [mmcv.imresize(img, (new_w, new_h))\n",
    "                for img in results['imgs']]\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoCrop(BaseTransform):\n",
    "    def __init__(self, c_size):\n",
    "        self.c_size = c_size\n",
    "\n",
    "    def transform(self, results):\n",
    "        img_h, img_w = results['img_shape']\n",
    "        center_x, center_y = img_w // 2, img_h // 2\n",
    "        x1, x2 = center_x - self.c_size // 2, center_x + self.c_size // 2\n",
    "        y1, y2 = center_y - self.c_size // 2, center_y + self.c_size // 2\n",
    "        imgs = [img[y1:y2, x1:x2] for img in results['imgs']]\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoFormat(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        num_clips = results['num_clips']\n",
    "        clip_len = results['clip_len']\n",
    "        imgs = results['imgs']\n",
    "\n",
    "        # [num_clips*clip_len, H, W, C]\n",
    "        imgs = np.array(imgs)\n",
    "        # [num_clips, clip_len, H, W, C]\n",
    "        imgs = imgs.reshape((num_clips, clip_len) + imgs.shape[1:])\n",
    "        # [num_clips, C, clip_len, H, W]\n",
    "        imgs = imgs.transpose(0, 4, 1, 2, 3)\n",
    "\n",
    "        results['imgs'] = imgs\n",
    "        return results\n",
    "    \n",
    "@TRANSFORMS.register_module()\n",
    "class VideoPack(BaseTransform):\n",
    "    def __init__(self, meta_keys=('img_shape', 'num_clips', 'clip_len')):\n",
    "        self.meta_keys = meta_keys\n",
    "\n",
    "    def transform(self, results):\n",
    "        packed_results = dict()\n",
    "        inputs = to_tensor(results['imgs'])\n",
    "        data_sample = ActionDataSample()\n",
    "        data_sample.set_gt_label(results['label'])\n",
    "        metainfo = {k: results[k] for k in self.meta_keys if k in results}\n",
    "        data_sample.set_metainfo(metainfo)\n",
    "        packed_results['inputs'] = inputs\n",
    "        packed_results['data_samples'] = data_sample\n",
    "        return packed_results"
   ],
   "id": "c88287a5f1d8d066",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'VideoSample is already registered in transform at __main__'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;129;43m@TRANSFORMS\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mregister_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;43;01mclass\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43;01mVideoSample\u001B[39;49;00m\u001B[43m(\u001B[49m\u001B[43mBaseTransform\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_clips\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclip_len\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mclip_len\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages\\mmengine\\registry\\registry.py:666\u001B[0m, in \u001B[0;36mRegistry.register_module.<locals>._register\u001B[1;34m(module)\u001B[0m\n\u001B[0;32m    665\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_register\u001B[39m(module):\n\u001B[1;32m--> 666\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_register_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    667\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m module\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\openmmlab_v04\\lib\\site-packages\\mmengine\\registry\\registry.py:611\u001B[0m, in \u001B[0;36mRegistry._register_module\u001B[1;34m(self, module, module_name, force)\u001B[0m\n\u001B[0;32m    609\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m force \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_module_dict:\n\u001B[0;32m    610\u001B[0m     existed_module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule_dict[name]\n\u001B[1;32m--> 611\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is already registered in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    612\u001B[0m                    \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mat \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexisted_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_module_dict[name] \u001B[38;5;241m=\u001B[39m module\n",
      "\u001B[1;31mKeyError\u001B[0m: 'VideoSample is already registered in transform at __main__'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:53:56.441567Z",
     "start_time": "2024-09-06T17:53:56.400056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "from mmengine.dataset import Compose\n",
    "\n",
    "pipeline_cfg = [\n",
    "    dict(type='VideoInit'),\n",
    "    dict(type='VideoSample', clip_len=16, num_clips=1, test_mode=False),\n",
    "    dict(type='VideoDecode'),\n",
    "    dict(type='VideoResize', r_size=256),\n",
    "    dict(type='VideoCrop', c_size=224),\n",
    "    dict(type='VideoFormat'),\n",
    "    dict(type='VideoPack')\n",
    "]\n",
    "\n",
    "pipeline = Compose(pipeline_cfg)\n",
    "\n",
    "\n",
    "data_prefix = \"C:\\\\Users\\\\rodol\\\\mmaction2\\\\data\\\\kinetics400_tiny\\\\train\"\n",
    "results = dict(filename=osp.join(data_prefix, 'D32_1gwq35E.mp4'), label=0)\n",
    "packed_results = pipeline(results)\n",
    "\n",
    "inputs = packed_results['inputs']\n",
    "data_sample = packed_results['data_samples']\n",
    "\n",
    "print('shape of the inputs: ', inputs.shape)\n",
    "\n",
    "# Get metainfo of the inputs\n",
    "print('image_shape: ', data_sample.img_shape)\n",
    "print('num_clips: ', data_sample.num_clips)\n",
    "print('clip_len: ', data_sample.clip_len)\n",
    "\n",
    "# Get label of the inputs\n",
    "print('label: ', data_sample.gt_label)"
   ],
   "id": "f0d2dae55c71beed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the inputs:  torch.Size([1, 3, 16, 224, 224])\n",
      "image_shape:  (224, 224)\n",
      "num_clips:  1\n",
      "clip_len:  16\n",
      "label:  tensor([0])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:53:57.627191Z",
     "start_time": "2024-09-06T17:53:57.615679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "from mmengine.fileio import list_from_file\n",
    "from mmengine.dataset import BaseDataset\n",
    "from mmaction.registry import DATASETS\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class DatasetZelda(BaseDataset):\n",
    "    def __init__(self, ann_file, pipeline, data_root, data_prefix=dict(video=''),\n",
    "                 test_mode=False, modality='RGB', **kwargs):\n",
    "        self.modality = modality\n",
    "        super(DatasetZelda, self).__init__(ann_file=ann_file, pipeline=pipeline, data_root=data_root,\n",
    "                                           data_prefix=data_prefix, test_mode=test_mode,\n",
    "                                           **kwargs)\n",
    "\n",
    "    def load_data_list(self):\n",
    "        data_list = []\n",
    "        fin = list_from_file(self.ann_file)\n",
    "        for line in fin:\n",
    "            line_split = line.strip().split()\n",
    "            filename, label = line_split\n",
    "            label = int(label)\n",
    "            filename = osp.join(self.data_prefix['video'], filename)\n",
    "            data_list.append(dict(filename=filename, label=label))\n",
    "        return data_list\n",
    "\n",
    "    def get_data_info(self, idx: int) -> dict:\n",
    "        data_info = super().get_data_info(idx)\n",
    "        data_info['modality'] = self.modality\n",
    "        return data_info"
   ],
   "id": "4466626b1f98153a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:53:59.481532Z",
     "start_time": "2024-09-06T17:53:59.463534Z"
    }
   },
   "cell_type": "code",
   "source": "root_dir = \"C:\\\\Users\\\\rodol\\\\mmaction2\\\\data\\\\kinetics400_tiny\\\\\"",
   "id": "b2f94fb05e03130c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:54:02.206591Z",
     "start_time": "2024-09-06T17:54:01.914875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mmaction.registry import DATASETS\n",
    "\n",
    "train_pipeline_cfg = [\n",
    "    dict(type='VideoInit'),\n",
    "    dict(type='VideoSample', clip_len=16, num_clips=1, test_mode=False),\n",
    "    dict(type='VideoDecode'),\n",
    "    dict(type='VideoResize', r_size=256),\n",
    "    dict(type='VideoCrop', c_size=224),\n",
    "    dict(type='VideoFormat'),\n",
    "    dict(type='VideoPack')\n",
    "]\n",
    "\n",
    "val_pipeline_cfg = [\n",
    "    dict(type='VideoInit'),\n",
    "    dict(type='VideoSample', clip_len=16, num_clips=5, test_mode=True),\n",
    "    dict(type='VideoDecode'),\n",
    "    dict(type='VideoResize', r_size=256),\n",
    "    dict(type='VideoCrop', c_size=224),\n",
    "    dict(type='VideoFormat'),\n",
    "    dict(type='VideoPack')\n",
    "]\n",
    "\n",
    "train_dataset_cfg = dict(\n",
    "    type='DatasetZelda',\n",
    "    ann_file='kinetics_tiny_train_video.txt',\n",
    "    pipeline=train_pipeline_cfg,\n",
    "    data_root= root_dir,\n",
    "    data_prefix=dict(video='train'))\n",
    "\n",
    "val_dataset_cfg = dict(\n",
    "    type='DatasetZelda',\n",
    "    ann_file='kinetics_tiny_val_video.txt',\n",
    "    pipeline=val_pipeline_cfg,\n",
    "    data_root= root_dir,\n",
    "    data_prefix=dict(video='val'))\n",
    "\n",
    "train_dataset = DATASETS.build(train_dataset_cfg)\n",
    "\n",
    "packed_results = train_dataset[0]\n",
    "\n",
    "inputs = packed_results['inputs']\n",
    "data_sample = packed_results['data_samples']\n",
    "\n",
    "print('shape of the inputs: ', inputs.shape)\n",
    "\n",
    "# Get metainfo of the inputs\n",
    "print('image_shape: ', data_sample.img_shape)\n",
    "print('num_clips: ', data_sample.num_clips)\n",
    "print('clip_len: ', data_sample.clip_len)\n",
    "\n",
    "# Get label of the inputs\n",
    "print('label: ', data_sample.gt_label)\n",
    "\n",
    "\n",
    "\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataloader_cfg = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
    "    dataset=train_dataset_cfg)\n",
    "\n",
    "val_dataloader_cfg = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=val_dataset_cfg)\n",
    "\n",
    "train_data_loader = Runner.build_dataloader(dataloader=train_dataloader_cfg)\n",
    "val_data_loader = Runner.build_dataloader(dataloader=val_dataloader_cfg)\n",
    "\n",
    "batched_packed_results = next(iter(train_data_loader))\n",
    "\n",
    "batched_inputs = batched_packed_results['inputs']\n",
    "batched_data_sample = batched_packed_results['data_samples']\n",
    "\n",
    "assert len(batched_inputs) == BATCH_SIZE\n",
    "assert len(batched_data_sample) == BATCH_SIZE"
   ],
   "id": "4ef0b55a25e6adb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the inputs:  torch.Size([1, 3, 16, 224, 224])\n",
      "image_shape:  (224, 224)\n",
      "num_clips:  1\n",
      "clip_len:  16\n",
      "label:  tensor([0])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:54:04.046197Z",
     "start_time": "2024-09-06T17:54:04.029198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from mmengine.model import BaseDataPreprocessor, stack_batch\n",
    "from mmaction.registry import MODELS\n",
    "\n",
    "@MODELS.register_module()\n",
    "class DataPreprocessorZelda(BaseDataPreprocessor):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\n",
    "            'mean',\n",
    "            torch.tensor(mean, dtype=torch.float32).view(-1, 1, 1, 1),\n",
    "            False)\n",
    "        self.register_buffer(\n",
    "            'std',\n",
    "            torch.tensor(std, dtype=torch.float32).view(-1, 1, 1, 1),\n",
    "            False)\n",
    "\n",
    "    def forward(self, data, training=False):\n",
    "        data = self.cast_data(data)\n",
    "        inputs = data['inputs']\n",
    "        batch_inputs = stack_batch(inputs)  # Batching\n",
    "        batch_inputs = (batch_inputs - self.mean) / self.std  # Normalization\n",
    "        data['inputs'] = batch_inputs\n",
    "        return data"
   ],
   "id": "b260406fe8a7bf04",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:54:06.628151Z",
     "start_time": "2024-09-06T17:54:06.606641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mmaction.registry import MODELS\n",
    "\n",
    "data_preprocessor_cfg = dict(\n",
    "    type='DataPreprocessorZelda',\n",
    "    mean=[123.675, 116.28, 103.53],\n",
    "    std=[58.395, 57.12, 57.375])\n",
    "\n",
    "data_preprocessor = MODELS.build(data_preprocessor_cfg)\n",
    "\n",
    "preprocessed_inputs = data_preprocessor(batched_packed_results)\n",
    "print(preprocessed_inputs['inputs'].shape)"
   ],
   "id": "70cd1f50a5afb19b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3, 16, 224, 224])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:54:08.391977Z",
     "start_time": "2024-09-06T17:54:08.365977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mmengine.model import BaseModel, BaseModule, Sequential\n",
    "from mmengine.structures import LabelData\n",
    "from mmaction.registry import MODELS\n",
    "\n",
    "@MODELS.register_module()\n",
    "class BackBoneZelda(BaseModule):\n",
    "    def __init__(self, init_cfg=None):\n",
    "        if init_cfg is None:\n",
    "            init_cfg = [dict(type='Kaiming', layer='Conv3d', mode='fan_out', nonlinearity=\"relu\"),\n",
    "                        dict(type='Constant', layer='BatchNorm3d', val=1, bias=0)]\n",
    "\n",
    "        super(BackBoneZelda, self).__init__(init_cfg=init_cfg)\n",
    "\n",
    "        self.conv1 = Sequential(nn.Conv3d(3, 64, kernel_size=(3, 7, 7),\n",
    "                                          stride=(1, 2, 2), padding=(1, 3, 3)),\n",
    "                                nn.BatchNorm3d(64), nn.ReLU())\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2),\n",
    "                                    padding=(0, 1, 1))\n",
    "\n",
    "        self.conv = Sequential(nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                               nn.BatchNorm3d(128), nn.ReLU())\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        # imgs: [batch_size*num_views, 3, T, H, W]\n",
    "        # features: [batch_size*num_views, 128, T/2, H//8, W//8]\n",
    "        features = self.conv(self.maxpool(self.conv1(imgs)))\n",
    "        return features\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class ClsHeadZelda(BaseModule):\n",
    "    def __init__(self, num_classes, in_channels, dropout=0.5, average_clips='prob', init_cfg=None):\n",
    "        if init_cfg is None:\n",
    "            init_cfg = dict(type='Normal', layer='Linear', std=0.01)\n",
    "\n",
    "        super(ClsHeadZelda, self).__init__(init_cfg=init_cfg)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.average_clips = average_clips\n",
    "\n",
    "        if dropout != 0:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "        self.fc = nn.Linear(self.in_channels, self.num_classes)\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, T, H, W = x.shape\n",
    "        x = self.pool(x)\n",
    "        x = x.view(N, C)\n",
    "        assert x.shape[1] == self.in_channels\n",
    "\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        cls_scores = self.fc(x)\n",
    "        return cls_scores\n",
    "\n",
    "    def loss(self, feats, data_samples):\n",
    "        cls_scores = self(feats)\n",
    "        labels = torch.stack([x.gt_label for x in data_samples])\n",
    "        labels = labels.squeeze()\n",
    "\n",
    "        if labels.shape == torch.Size([]):\n",
    "            labels = labels.unsqueeze(0)\n",
    "\n",
    "        loss_cls = self.loss_fn(cls_scores, labels)\n",
    "        return dict(loss_cls=loss_cls)\n",
    "\n",
    "    def predict(self, feats, data_samples):\n",
    "        cls_scores = self(feats)\n",
    "        num_views = cls_scores.shape[0] // len(data_samples)\n",
    "        # assert num_views == data_samples[0].num_clips\n",
    "        cls_scores = self.average_clip(cls_scores, num_views)\n",
    "\n",
    "        for ds, sc in zip(data_samples, cls_scores):\n",
    "            pred = LabelData(item=sc)\n",
    "            ds.pred_scores = pred\n",
    "        return data_samples\n",
    "\n",
    "    def average_clip(self, cls_scores, num_views):\n",
    "          if self.average_clips not in ['score', 'prob', None]:\n",
    "            raise ValueError(f'{self.average_clips} is not supported. '\n",
    "                             f'Currently supported ones are '\n",
    "                             f'[\"score\", \"prob\", None]')\n",
    "\n",
    "          total_views = cls_scores.shape[0]\n",
    "          cls_scores = cls_scores.view(total_views // num_views, num_views, -1)\n",
    "\n",
    "          if self.average_clips is None:\n",
    "              return cls_scores\n",
    "          elif self.average_clips == 'prob':\n",
    "              cls_scores = F.softmax(cls_scores, dim=2).mean(dim=1)\n",
    "          elif self.average_clips == 'score':\n",
    "              cls_scores = cls_scores.mean(dim=1)\n",
    "\n",
    "          return cls_scores\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class RecognizerZelda(BaseModel):\n",
    "    def __init__(self, backbone, cls_head, data_preprocessor):\n",
    "        super().__init__(data_preprocessor=data_preprocessor)\n",
    "\n",
    "        self.backbone = MODELS.build(backbone)\n",
    "        self.cls_head = MODELS.build(cls_head)\n",
    "\n",
    "    def extract_feat(self, inputs):\n",
    "        inputs = inputs.view((-1, ) + inputs.shape[2:])\n",
    "        return self.backbone(inputs)\n",
    "\n",
    "    def loss(self, inputs, data_samples):\n",
    "        feats = self.extract_feat(inputs)\n",
    "        loss = self.cls_head.loss(feats, data_samples)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, inputs, data_samples):\n",
    "        feats = self.extract_feat(inputs)\n",
    "        predictions = self.cls_head.predict(feats, data_samples)\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, inputs, data_samples=None, mode='tensor'):\n",
    "        if mode == 'tensor':\n",
    "            return self.extract_feat(inputs)\n",
    "        elif mode == 'loss':\n",
    "            return self.loss(inputs, data_samples)\n",
    "        elif mode == 'predict':\n",
    "            return self.predict(inputs, data_samples)\n",
    "        else:\n",
    "            raise RuntimeError(f'Invalid mode: {mode}')"
   ],
   "id": "b4464595b6a52a99",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:54:11.643748Z",
     "start_time": "2024-09-06T17:54:11.335199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import copy\n",
    "from mmaction.registry import MODELS\n",
    "\n",
    "model_cfg = dict(\n",
    "    type='RecognizerZelda',\n",
    "    backbone=dict(type='BackBoneZelda'),\n",
    "    cls_head=dict(\n",
    "        type='ClsHeadZelda',\n",
    "        num_classes=2,\n",
    "        in_channels=128,\n",
    "        average_clips='prob'),\n",
    "    data_preprocessor = dict(\n",
    "        type='DataPreprocessorZelda',\n",
    "        mean=[123.675, 116.28, 103.53],\n",
    "        std=[58.395, 57.12, 57.375]))\n",
    "\n",
    "model = MODELS.build(model_cfg)\n",
    "\n",
    "# Train\n",
    "model.train()\n",
    "model.init_weights()\n",
    "data_batch_train = copy.deepcopy(batched_packed_results)\n",
    "data = model.data_preprocessor(data_batch_train, training=True)\n",
    "loss = model(**data, mode='loss')\n",
    "print('loss dict: ', loss)\n",
    "\n",
    "# Test\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    data_batch_test = copy.deepcopy(batched_packed_results)\n",
    "    data = model.data_preprocessor(data_batch_test, training=False)\n",
    "    predictions = model(**data, mode='predict')\n",
    "print('Label of Sample[0]', predictions[0].gt_label)\n",
    "print('Scores of Sample[0]', predictions[0].to_dict()[\"pred_scores\"][\"item\"])"
   ],
   "id": "b582f132ae1abda9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/06 12:54:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "backbone.conv1.0.weight - torch.Size([64, 3, 3, 7, 7]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "09/06 12:54:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "backbone.conv1.0.bias - torch.Size([64]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "09/06 12:54:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "backbone.conv1.1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
      " \n",
      "09/06 12:54:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "backbone.conv1.1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
      " \n",
      "09/06 12:54:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "backbone.conv.0.weight - torch.Size([128, 64, 3, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "09/06 12:54:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "backbone.conv.0.bias - torch.Size([128]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "09/06 12:54:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "backbone.conv.1.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
      " \n",
      "09/06 12:54:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "backbone.conv.1.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
      " \n",
      "09/06 12:54:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "cls_head.fc.weight - torch.Size([2, 128]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/06 12:54:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "cls_head.fc.bias - torch.Size([2]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "loss dict:  {'loss_cls': tensor(0.6924, grad_fn=<NllLossBackward0>)}\n",
      "Label of Sample[0] tensor([1])\n",
      "Scores of Sample[0] tensor([0.4974, 0.5026])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ignore the following cells (START)\n",
    "The following cells are simply to get to know mmaction2"
   ],
   "id": "2e7d07b7ba5b00ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T21:19:29.058250Z",
     "start_time": "2024-09-05T21:19:29.039250Z"
    }
   },
   "cell_type": "code",
   "source": "dir(predictions[0])",
   "id": "c95a73a11222d29b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_data_fields',\n",
       " '_metainfo_fields',\n",
       " 'all_items',\n",
       " 'all_keys',\n",
       " 'all_values',\n",
       " 'clip_len',\n",
       " 'clone',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'detach',\n",
       " 'features',\n",
       " 'get',\n",
       " 'gt_instances',\n",
       " 'gt_label',\n",
       " 'img_shape',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'metainfo',\n",
       " 'metainfo_items',\n",
       " 'metainfo_keys',\n",
       " 'metainfo_values',\n",
       " 'mlu',\n",
       " 'musa',\n",
       " 'new',\n",
       " 'npu',\n",
       " 'num_clips',\n",
       " 'numpy',\n",
       " 'pop',\n",
       " 'pred_scores',\n",
       " 'proposals',\n",
       " 'set_data',\n",
       " 'set_field',\n",
       " 'set_gt_label',\n",
       " 'set_metainfo',\n",
       " 'set_pred_label',\n",
       " 'set_pred_score',\n",
       " 'to',\n",
       " 'to_dict',\n",
       " 'to_tensor',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T21:20:36.269815Z",
     "start_time": "2024-09-05T21:20:36.249816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# copy the predictions[0] variable to temp\n",
    "temp = copy.deepcopy(predictions[0])\n",
    "temp.to_dict()"
   ],
   "id": "f81b954496782042",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clip_len': 16,\n",
       " 'num_clips': 1,\n",
       " 'img_shape': (224, 224),\n",
       " 'gt_label': tensor([1]),\n",
       " 'pred_scores': {'item': tensor([0.4975, 0.5025])}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T21:21:46.805436Z",
     "start_time": "2024-09-05T21:21:46.799436Z"
    }
   },
   "cell_type": "code",
   "source": "temp.to_dict()[\"pred_scores\"][\"item\"]",
   "id": "cb0af383f3aaf19",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4975, 0.5025])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T21:40:59.010714Z",
     "start_time": "2024-09-05T21:40:59.002715Z"
    }
   },
   "cell_type": "code",
   "source": "temp.to_dict()[\"pred_scores\"][\"item\"].numpy()",
   "id": "51dd295534e8d6d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49752143, 0.5024786 ], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ignore the following cells (END)",
   "id": "83a77af29daa1dd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:54:37.653972Z",
     "start_time": "2024-09-06T17:54:37.637458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy \n",
    "from collections import OrderedDict\n",
    "from mmengine.evaluator import BaseMetric\n",
    "from mmaction.evaluation import top_k_accuracy\n",
    "from mmaction.registry import METRICS\n",
    "\n",
    "@METRICS.register_module()\n",
    "class AccuracyMetric_c(BaseMetric):\n",
    "    def __init__(self, topk=(1, 5), collect_device='cpu', prefix='acc'):\n",
    "        super().__init__(collect_device=collect_device, prefix=prefix)\n",
    "        self.topk = topk\n",
    "\n",
    "    def process(self, data_batch, data_samples):\n",
    "        data_samples = copy.deepcopy(data_samples)\n",
    "        for data_sample in data_samples:\n",
    "            result = dict()\n",
    "            scores = data_sample['pred_scores']['item'].cpu().numpy() # if you get errors downstream of this cell, try changine 'pred_score' to 'pred_scores'\n",
    "            label = data_sample['gt_label'].item()\n",
    "            result['scores'] = scores\n",
    "            result['label'] = label\n",
    "            self.results.append(result)\n",
    "\n",
    "    def compute_metrics(self, results: list) -> dict:\n",
    "        eval_results = OrderedDict()\n",
    "        labels = [res['label'] for res in results]\n",
    "        scores = [res['scores'] for res in results]\n",
    "        topk_acc = top_k_accuracy(scores, labels, self.topk)\n",
    "        for k, acc in zip(self.topk, topk_acc):\n",
    "            eval_results[f'topk{k}'] = acc\n",
    "        return eval_results"
   ],
   "id": "b5de9ffbf2e5ecd2",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:54:40.618107Z",
     "start_time": "2024-09-06T17:54:40.605105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mmaction.registry import METRICS\n",
    "\n",
    "metric_cfg = dict(type='AccuracyMetric_c', topk=(1, 5))\n",
    "\n",
    "metric = METRICS.build(metric_cfg)\n",
    "\n",
    "data_samples = [d.to_dict() for d in predictions]\n",
    "\n",
    "metric.process(batched_packed_results, data_samples)\n",
    "acc = metric.compute_metrics(metric.results)\n",
    "print(acc)"
   ],
   "id": "1f758125e408f461",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('topk1', 1.0), ('topk5', 1.0)])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:54:43.899725Z",
     "start_time": "2024-09-06T17:54:43.881726Z"
    }
   },
   "cell_type": "code",
   "source": "save_directory = \"C:\\\\Users\\\\rodol\\\\Desktop\\\\mmaction2_test\"",
   "id": "1e9cdf79df3cb786",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:55:19.587765Z",
     "start_time": "2024-09-06T17:54:47.638077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mmengine.runner import Runner\n",
    "\n",
    "train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=10, val_interval=1)\n",
    "val_cfg = dict(type='ValLoop')\n",
    "\n",
    "optim_wrapper = dict(optimizer=dict(type='Adam', lr=0.01))\n",
    "\n",
    "runner = Runner(model=model_cfg, work_dir= save_directory,\n",
    "                train_dataloader=train_dataloader_cfg,\n",
    "                train_cfg=train_cfg,\n",
    "                val_dataloader=val_dataloader_cfg,\n",
    "                val_cfg=val_cfg,\n",
    "                optim_wrapper=optim_wrapper,\n",
    "                val_evaluator=[metric_cfg],\n",
    "                default_scope='mmaction')\n",
    "runner.train()"
   ],
   "id": "3273150060de8284",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/06 12:54:48 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: win32\n",
      "    Python: 3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 449743703\n",
      "    GPU 0: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "    CUDA_HOME: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\n",
      "    NVCC: Cuda compilation tools, release 11.8, V11.8.89\n",
      "    MSVC: n/a, reason: fileno\n",
      "    PyTorch: 2.0.1\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 193431937\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 2019\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.8\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.7\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj /FS -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=OFF, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.15.2\n",
      "    OpenCV: 4.10.0\n",
      "    MMEngine: 0.10.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 449743703\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "09/06 12:54:48 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "09/06 12:54:48 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "09/06 12:54:49 - mmengine - \u001B[5m\u001B[4m\u001B[33mWARNING\u001B[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "09/06 12:54:49 - mmengine - \u001B[5m\u001B[4m\u001B[33mWARNING\u001B[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "09/06 12:54:49 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Checkpoints will be saved to C:\\Users\\rodol\\Desktop\\mmaction2_test.\n",
      "09/06 12:54:50 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(train)  [1][10/15]  lr: 1.0000e-02  eta: 0:00:14  time: 0.1051  data_time: 0.0624  memory: 430  loss: 0.8041  loss_cls: 0.8041\n",
      "09/06 12:54:50 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Exp name: 20240906_125447\n",
      "09/06 12:54:50 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Saving checkpoint at 1 epochs\n",
      "09/06 12:54:50 - mmengine - \u001B[5m\u001B[4m\u001B[33mWARNING\u001B[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "09/06 12:54:52 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(val) [1][5/5]    acc/topk1: 0.5000  acc/topk5: 1.0000  data_time: 0.2646  time: 0.2829\n",
      "09/06 12:54:52 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(train)  [2][10/15]  lr: 1.0000e-02  eta: 0:00:11  time: 0.0796  data_time: 0.0507  memory: 1090  loss: 0.8276  loss_cls: 0.8276\n",
      "09/06 12:54:53 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Exp name: 20240906_125447\n",
      "09/06 12:54:53 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Saving checkpoint at 2 epochs\n",
      "09/06 12:54:54 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(val) [2][5/5]    acc/topk1: 0.5000  acc/topk5: 1.0000  data_time: 0.2630  time: 0.2788\n",
      "09/06 12:54:55 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(train)  [3][10/15]  lr: 1.0000e-02  eta: 0:00:09  time: 0.0792  data_time: 0.0502  memory: 1090  loss: 0.8046  loss_cls: 0.8046\n",
      "09/06 12:54:55 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Exp name: 20240906_125447\n",
      "09/06 12:54:55 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Saving checkpoint at 3 epochs\n",
      "09/06 12:54:57 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(val) [3][5/5]    acc/topk1: 0.3000  acc/topk5: 1.0000  data_time: 0.2623  time: 0.2800\n",
      "09/06 12:54:58 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(train)  [4][10/15]  lr: 1.0000e-02  eta: 0:00:08  time: 0.0976  data_time: 0.0573  memory: 1090  loss: 0.5690  loss_cls: 0.5690\n",
      "09/06 12:54:58 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Exp name: 20240906_125447\n",
      "09/06 12:54:58 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Saving checkpoint at 4 epochs\n",
      "09/06 12:55:00 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(val) [4][5/5]    acc/topk1: 0.4000  acc/topk5: 1.0000  data_time: 0.2970  time: 0.3252\n",
      "09/06 12:55:01 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(train)  [5][10/15]  lr: 1.0000e-02  eta: 0:00:07  time: 0.0974  data_time: 0.0528  memory: 1090  loss: 0.5956  loss_cls: 0.5956\n",
      "09/06 12:55:02 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Exp name: 20240906_125447\n",
      "09/06 12:55:02 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Saving checkpoint at 5 epochs\n",
      "09/06 12:55:03 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(val) [5][5/5]    acc/topk1: 0.4000  acc/topk5: 1.0000  data_time: 0.3043  time: 0.3337\n",
      "09/06 12:55:04 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(train)  [6][10/15]  lr: 1.0000e-02  eta: 0:00:05  time: 0.0969  data_time: 0.0520  memory: 1090  loss: 0.4858  loss_cls: 0.4858\n",
      "09/06 12:55:05 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Exp name: 20240906_125447\n",
      "09/06 12:55:05 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Saving checkpoint at 6 epochs\n",
      "09/06 12:55:06 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(val) [6][5/5]    acc/topk1: 0.4000  acc/topk5: 1.0000  data_time: 0.3113  time: 0.3407\n",
      "09/06 12:55:07 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(train)  [7][10/15]  lr: 1.0000e-02  eta: 0:00:04  time: 0.1040  data_time: 0.0573  memory: 1090  loss: 0.9650  loss_cls: 0.9650\n",
      "09/06 12:55:08 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Exp name: 20240906_125447\n",
      "09/06 12:55:08 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Saving checkpoint at 7 epochs\n",
      "09/06 12:55:10 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(val) [7][5/5]    acc/topk1: 0.5000  acc/topk5: 1.0000  data_time: 0.3029  time: 0.3325\n",
      "09/06 12:55:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(train)  [8][10/15]  lr: 1.0000e-02  eta: 0:00:03  time: 0.0976  data_time: 0.0548  memory: 1090  loss: 0.6130  loss_cls: 0.6130\n",
      "09/06 12:55:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Exp name: 20240906_125447\n",
      "09/06 12:55:11 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Saving checkpoint at 8 epochs\n",
      "09/06 12:55:13 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(val) [8][5/5]    acc/topk1: 0.6000  acc/topk5: 1.0000  data_time: 0.3001  time: 0.3300\n",
      "09/06 12:55:14 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(train)  [9][10/15]  lr: 1.0000e-02  eta: 0:00:01  time: 0.1008  data_time: 0.0540  memory: 1090  loss: 0.6066  loss_cls: 0.6066\n",
      "09/06 12:55:14 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Exp name: 20240906_125447\n",
      "09/06 12:55:14 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Saving checkpoint at 9 epochs\n",
      "09/06 12:55:16 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(val) [9][5/5]    acc/topk1: 0.4000  acc/topk5: 1.0000  data_time: 0.2963  time: 0.3269\n",
      "09/06 12:55:17 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(train) [10][10/15]  lr: 1.0000e-02  eta: 0:00:00  time: 0.1015  data_time: 0.0531  memory: 1090  loss: 0.6780  loss_cls: 0.6780\n",
      "09/06 12:55:17 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Exp name: 20240906_125447\n",
      "09/06 12:55:17 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Saving checkpoint at 10 epochs\n",
      "09/06 12:55:19 - mmengine - \u001B[4m\u001B[97mINFO\u001B[0m - Epoch(val) [10][5/5]    acc/topk1: 0.5000  acc/topk5: 1.0000  data_time: 0.2977  time: 0.3288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RecognizerZelda(\n",
       "  (data_preprocessor): DataPreprocessorZelda()\n",
       "  (backbone): BackBoneZelda(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3))\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (maxpool): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
       "    (conv): Sequential(\n",
       "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv3d', 'mode': 'fan_out', 'nonlinearity': 'relu'}, {'type': 'Constant', 'layer': 'BatchNorm3d', 'val': 1, 'bias': 0}]\n",
       "  (cls_head): ClsHeadZelda(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (pool): AdaptiveAvgPool3d(output_size=1)\n",
       "    (loss_fn): CrossEntropyLoss()\n",
       "  )\n",
       "  init_cfg={'type': 'Normal', 'layer': 'Linear', 'std': 0.01}\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T20:31:37.017466Z",
     "start_time": "2024-09-06T20:31:34.245290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mmaction.apis import inference_recognizer, init_recognizer\n",
    "config_path = r\"C:\\Users\\rodol\\mmaction2\\configs\\recognition\\tsn\\tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb.py\"\n",
    "checkpoint_path = 'https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb_20220906-2692d16c.pth'\n",
    "# img_path = \"C:/Users/rodol/Downloads/mmaction2/mmaction2/demo/demo.mp4\"\n",
    "img_path = r\"C:\\Users\\rodol\\Desktop\\E2E_Animation\\mp4_videos\\myle_salsa_dance_right_1.mp4\"\n",
    "\n",
    "\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_recognizer(config_path, checkpoint_path, device=\"cuda:0\")  # device can be 'cuda:0'\n",
    "# test a single image\n",
    "result = inference_recognizer(model, img_path)"
   ],
   "id": "7c3349d046bf9e49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb_20220906-2692d16c.pth\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T20:31:51.026974Z",
     "start_time": "2024-09-06T20:31:51.005974Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "72885b9f4d80534b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActionDataSample(\n",
       "\n",
       "    META INFORMATION\n",
       "    img_shape: (224, 224)\n",
       "    num_classes: 400\n",
       "\n",
       "    DATA FIELDS\n",
       "    gt_label: tensor([-1], device='cuda:0')\n",
       "    pred_score: tensor([2.1894e-05, 6.0890e-05, 9.0677e-07, 1.8772e-06, 2.6567e-07, 8.4372e-06,\n",
       "                1.1853e-07, 6.1647e-07, 3.2694e-09, 1.9830e-06, 2.5955e-08, 3.3746e-07,\n",
       "                1.0267e-05, 3.7873e-08, 2.6045e-08, 3.7008e-07, 7.6685e-04, 1.0057e-08,\n",
       "                4.8864e-04, 1.7828e-07, 1.7355e-04, 7.2847e-08, 1.4406e-08, 1.7917e-08,\n",
       "                1.8651e-06, 1.5357e-06, 1.0317e-06, 1.1868e-07, 1.1899e-08, 4.8263e-09,\n",
       "                9.0950e-08, 8.8833e-04, 2.3220e-08, 8.9272e-08, 7.0239e-03, 3.1559e-08,\n",
       "                1.6723e-07, 1.5605e-07, 4.7817e-07, 5.9423e-08, 1.1296e-09, 2.6783e-04,\n",
       "                1.5621e-07, 8.5514e-04, 1.0160e-05, 7.0143e-04, 3.5789e-07, 9.3122e-08,\n",
       "                4.1398e-05, 1.4509e-03, 6.7694e-04, 3.6674e-07, 1.5850e-08, 3.6335e-08,\n",
       "                1.8279e-08, 6.7639e-06, 1.3834e-08, 5.8579e-06, 9.4770e-08, 1.8669e-06,\n",
       "                3.6538e-04, 2.7485e-08, 2.8221e-08, 9.9207e-07, 3.6891e-07, 2.8210e-06,\n",
       "                5.7963e-07, 1.4726e-07, 4.2958e-08, 9.3558e-02, 2.4838e-07, 8.8592e-07,\n",
       "                2.7875e-09, 1.8873e-08, 6.1855e-07, 1.5764e-01, 3.1749e-06, 1.0332e-05,\n",
       "                4.7392e-08, 1.2975e-07, 5.5820e-08, 3.2410e-08, 5.7844e-07, 1.8502e-07,\n",
       "                1.6659e-02, 9.1146e-03, 6.3881e-04, 8.9597e-05, 3.7781e-06, 1.3345e-06,\n",
       "                2.9743e-08, 9.1474e-06, 1.3639e-05, 2.9053e-07, 8.2019e-04, 8.4926e-06,\n",
       "                7.4335e-08, 9.1484e-08, 4.7513e-08, 1.0427e-04, 2.6875e-06, 1.5735e-06,\n",
       "                4.5382e-07, 6.4435e-07, 2.7192e-08, 1.3455e-05, 4.3950e-07, 4.8354e-06,\n",
       "                1.1386e-07, 1.5283e-06, 1.4391e-08, 3.4843e-07, 4.7040e-06, 1.6607e-06,\n",
       "                1.1815e-07, 1.8413e-06, 5.2955e-07, 4.2197e-07, 1.1934e-06, 1.6867e-05,\n",
       "                4.7846e-06, 3.7470e-06, 1.5938e-06, 3.0895e-06, 4.0943e-08, 1.6254e-08,\n",
       "                2.0595e-07, 5.3227e-05, 4.4268e-06, 9.7978e-05, 1.4530e-05, 4.0959e-07,\n",
       "                1.0595e-06, 1.2122e-06, 1.5341e-05, 1.3137e-08, 3.8609e-07, 3.1818e-06,\n",
       "                5.9280e-08, 2.8230e-09, 2.5869e-06, 2.6651e-05, 2.7894e-06, 1.9198e-05,\n",
       "                7.0200e-08, 7.8227e-09, 3.5552e-07, 1.0378e-06, 2.3247e-07, 1.1807e-06,\n",
       "                2.9215e-07, 6.6792e-06, 2.3199e-04, 2.6151e-07, 3.9785e-05, 3.1400e-07,\n",
       "                2.3034e-05, 1.6188e-03, 1.7621e-04, 2.4698e-04, 2.1598e-06, 1.7993e-06,\n",
       "                1.7918e-07, 6.2918e-08, 3.0616e-06, 1.6587e-07, 5.1758e-07, 1.2731e-08,\n",
       "                3.1355e-04, 2.6003e-01, 6.2361e-05, 2.5496e-03, 1.4592e-07, 6.3004e-02,\n",
       "                1.5708e-07, 2.2096e-04, 1.9350e-05, 3.6495e-07, 1.1641e-07, 1.9133e-03,\n",
       "                4.0822e-07, 4.7931e-08, 2.2081e-06, 1.2138e-04, 1.3878e-08, 2.4622e-07,\n",
       "                9.0854e-06, 8.0684e-09, 1.3628e-07, 3.2913e-08, 1.0642e-08, 2.0908e-06,\n",
       "                1.6045e-06, 1.8499e-06, 4.1638e-07, 2.3868e-07, 7.1347e-07, 9.4114e-08,\n",
       "                2.2478e-04, 2.7459e-06, 5.4206e-06, 6.3153e-08, 7.3109e-06, 2.5856e-06,\n",
       "                1.0646e-08, 2.8961e-07, 2.9324e-07, 1.2260e-04, 4.5122e-08, 1.2981e-05,\n",
       "                6.7636e-08, 3.7815e-07, 3.2509e-08, 1.2672e-07, 2.3216e-08, 5.8094e-09,\n",
       "                1.0985e-06, 2.8214e-05, 1.8879e-03, 1.2205e-05, 3.2244e-05, 3.8292e-08,\n",
       "                4.5333e-08, 2.1502e-05, 1.8732e-06, 1.0330e-06, 7.6614e-09, 3.0097e-03,\n",
       "                1.1578e-06, 1.1022e-04, 6.3015e-08, 1.0260e-06, 6.0565e-07, 8.6323e-06,\n",
       "                2.5388e-05, 1.4247e-07, 4.9629e-07, 2.4210e-05, 5.0895e-08, 4.1631e-07,\n",
       "                7.2529e-08, 4.1044e-06, 2.7700e-08, 3.6122e-06, 2.2107e-04, 6.4122e-03,\n",
       "                1.1482e-05, 1.1130e-05, 5.2723e-06, 9.9558e-06, 2.2532e-05, 8.6394e-05,\n",
       "                1.4645e-06, 6.4478e-08, 6.6714e-08, 7.0837e-06, 3.8146e-07, 8.2331e-09,\n",
       "                3.7595e-07, 4.4301e-07, 1.7416e-05, 3.4994e-07, 2.6250e-06, 7.4403e-06,\n",
       "                1.4985e-07, 3.2579e-07, 5.9165e-06, 2.9549e-06, 1.5538e-06, 5.8614e-07,\n",
       "                1.2240e-07, 3.4100e-07, 4.0544e-07, 1.7784e-05, 5.3237e-05, 7.1742e-04,\n",
       "                3.3555e-08, 3.2952e-01, 1.4261e-06, 9.3561e-06, 1.8828e-04, 8.7879e-08,\n",
       "                6.9850e-08, 6.3466e-04, 1.8176e-04, 1.8067e-06, 3.0430e-07, 2.1644e-06,\n",
       "                8.1659e-06, 6.9045e-07, 2.5630e-08, 5.3726e-07, 1.2853e-07, 1.7328e-08,\n",
       "                9.5510e-09, 1.7954e-07, 1.2844e-04, 2.7750e-05, 1.4205e-05, 1.7919e-08,\n",
       "                2.9497e-08, 2.3980e-07, 1.8941e-04, 7.9154e-06, 2.3491e-04, 1.0358e-07,\n",
       "                9.9623e-04, 7.6926e-09, 6.0118e-08, 1.4505e-07, 1.1215e-09, 2.4415e-04,\n",
       "                2.4809e-08, 3.5424e-04, 3.9575e-05, 3.7456e-08, 1.7705e-05, 4.8788e-07,\n",
       "                3.8660e-06, 4.9498e-07, 5.3891e-07, 1.8724e-08, 4.3749e-07, 4.7124e-08,\n",
       "                4.1893e-09, 1.2382e-05, 2.8503e-03, 1.6861e-05, 6.0268e-06, 1.6726e-07,\n",
       "                1.0469e-05, 2.2215e-07, 4.3952e-09, 6.7829e-05, 4.7345e-05, 1.4455e-07,\n",
       "                3.6855e-09, 1.3682e-06, 1.3079e-04, 4.0618e-08, 5.0761e-08, 1.5620e-06,\n",
       "                9.2774e-05, 1.3215e-04, 5.8845e-08, 8.9614e-05, 1.8042e-02, 4.2475e-06,\n",
       "                4.4731e-04, 2.7089e-03, 2.9711e-08, 3.3624e-06, 4.2316e-07, 4.8631e-06,\n",
       "                7.3976e-08, 8.9483e-06, 6.9996e-06, 2.6416e-03, 1.1401e-05, 1.9668e-09,\n",
       "                2.8326e-08, 1.7428e-03, 3.2752e-08, 3.8552e-05, 5.1289e-07, 1.4743e-06,\n",
       "                2.4587e-08, 5.2632e-07, 1.1982e-07, 7.7932e-07, 5.9131e-08, 4.8789e-08,\n",
       "                2.5618e-07, 2.3987e-08, 4.1647e-07, 3.3563e-05, 6.0222e-09, 1.3566e-05,\n",
       "                2.2819e-04, 1.4148e-06, 6.7259e-08, 1.8148e-08, 5.5778e-06, 5.7522e-09,\n",
       "                5.0904e-07, 1.5915e-07, 2.2727e-08, 9.1794e-09, 1.1357e-08, 2.4006e-08,\n",
       "                6.6921e-08, 9.8390e-09, 1.6304e-06, 1.3674e-08, 8.5455e-08, 6.9072e-08,\n",
       "                1.4362e-07, 2.4307e-07, 3.2478e-04, 8.2854e-04], device='cuda:0')\n",
       "    pred_label: tensor([277], device='cuda:0')\n",
       ") at 0x19664838670>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T20:22:59.824175Z",
     "start_time": "2024-09-06T20:22:59.809175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "label = r\"C:\\Users\\rodol\\mmaction2\\tools\\data\\kinetics\\label_map_k400.txt\"\n",
    "\n",
    "pred_scores = result.pred_score.tolist()\n",
    "score_tuples = tuple(zip(range(len(pred_scores)), pred_scores))\n",
    "score_sorted = sorted(score_tuples, key=itemgetter(1), reverse=True)\n",
    "top5_label = score_sorted[:5]\n",
    "\n",
    "labels = open(label).readlines()\n",
    "labels = [x.strip() for x in labels]\n",
    "results = [(labels[x[0]], x[1]) for x in top5_label]\n",
    "\n",
    "print('The top-5 labels with corresponding scores are:')\n",
    "for result in results:\n",
    "    print(f'{result[0]}: ', result[1])"
   ],
   "id": "a4187a369b58bf05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-5 labels with corresponding scores are:\n",
      "robot dancing:  0.32952478528022766\n",
      "juggling balls:  0.26003003120422363\n",
      "country line dancing:  0.15764248371124268\n",
      "contact juggling:  0.09355822205543518\n",
      "jumpstyle dancing:  0.0630040094256401\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T20:31:18.540563Z",
     "start_time": "2024-09-06T20:31:18.532054Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "ab33d9e603103b1f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('robot dancing', 0.32952478528022766),\n",
       " ('juggling balls', 0.26003003120422363),\n",
       " ('country line dancing', 0.15764248371124268),\n",
       " ('contact juggling', 0.09355822205543518),\n",
       " ('jumpstyle dancing', 0.0630040094256401)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:49:41.800524Z",
     "start_time": "2024-09-06T17:49:41.779526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_path2 = \"C:\\\\Users\\\\rodol\\\\Pictures\\\\Screenshots\\\\rando.png\"\n",
    "img_test = mmcv.imread(img_path2)\n"
   ],
   "id": "b72c5c8e6ab4e50a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:51:08.625374Z",
     "start_time": "2024-09-06T17:51:05.990431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import mmcv\n",
    "img = np.random.rand(100, 100, 3)\n",
    "mmcv.imshow(img)"
   ],
   "id": "c5a2e9dce886ab17",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T18:13:02.553561Z",
     "start_time": "2024-09-06T18:12:59.214852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = r\"C:\\Users\\rodol\\Desktop\\E2E_Animation\\mp4_videos\\myle_salsa_dance_right_1.mp4\"\n",
    "mmcv.imshow(img_path2)# "
   ],
   "id": "fd54d0b3e036951b",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T17:48:15.600593Z",
     "start_time": "2024-09-06T17:48:15.532085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "# load image at path image_path2, and show the image\n",
    "image = cv2.imread(img_path2)\n",
    "cv2.imshow('image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "827f89b10aa79c55",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T18:30:01.480619Z",
     "start_time": "2024-09-06T18:30:01.360112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video = mmcv.VideoReader(img_path)\n",
    "# obtain basic information\n",
    "print(len(video))\n",
    "print(video.width, video.height, video.resolution, video.fps)"
   ],
   "id": "f4fe1c11789c23d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "3840 2160 (3840, 2160) 60.0\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T20:06:17.456099Z",
     "start_time": "2024-09-06T20:06:17.450099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label = r\"C:\\Users\\rodol\\mmaction2\\tools\\data\\kinetics\\label_map_k400.txt\"\n",
    "# open the file, go through each line, and append it to a variable called label_list\n",
    "label_list = []\n",
    "with open(label) as f:\n",
    "    for line in f:\n",
    "        label_list.append(line.strip())\n",
    "\n",
    "# print the first 10 labels\n",
    "print(label_list[:10])"
   ],
   "id": "a43afccf3ffb716c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abseiling', 'air drumming', 'answering questions', 'applauding', 'applying cream', 'archery', 'arm wrestling', 'arranging flowers', 'assembling computer', 'auctioning']\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T19:31:37.947607Z",
     "start_time": "2024-09-06T19:31:34.639050Z"
    }
   },
   "cell_type": "code",
   "source": "mmcv.imshow(img, \"check_it\")",
   "id": "1ead895122f56b5f",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T20:36:21.317653Z",
     "start_time": "2024-09-06T20:35:55.906642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import decord\n",
    "from pathlib import Path\n",
    "from mmaction.structures import ActionDataSample\n",
    "from mmaction.visualization import ActionVisualizer\n",
    "from mmengine.structures import LabelData\n",
    "path = r\"C:\\Users\\rodol\\Desktop\\E2E_Animation\\mp4_videos\\myle_salsa_dance_right_1.mp4\"\n",
    "video = decord.VideoReader(path)\n",
    "video = video.get_batch(range(32)).asnumpy()\n",
    "# data_sample is analogous to results in the previous example\n",
    "vis = ActionVisualizer(\n",
    "    save_dir=Path(\"C:\\\\Users\\\\rodol\\\\Desktop\\\\mmaction2_test\\\\outputs\"),\n",
    "    vis_backends= [dict(type='LocalVisBackend')]\n",
    ")\n",
    "vis.dataset_meta = {'classes': label_list}\n",
    "vis.add_datasample(\"check_it_out\", video, data_sample = result)\n",
    "assert Path(\"C:\\\\Users\\\\rodol\\\\Desktop\\\\mmaction2_test\\\\outputs\\\\vis_data\\\\check_it_out\\\\frames_0\\\\1.png\").exists()\n",
    "assert Path(\"C:\\\\Users\\\\rodol\\\\Desktop\\\\mmaction2_test\\\\outputs\\\\vis_data\\\\check_it_out\\\\frames_0\\\\2.png\").exists()\n",
    "vis.add_datasample(\"check_it_out\", video, step=1)\n",
    "assert Path(\"C:\\\\Users\\\\rodol\\\\Desktop\\\\mmaction2_test\\\\outputs\\\\vis_data\\\\check_it_out\\\\frames_1\\\\1.png\").exists()"
   ],
   "id": "3beb963243272d8c",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T20:21:50.002275Z",
     "start_time": "2024-09-06T20:21:49.995275Z"
    }
   },
   "cell_type": "code",
   "source": "label_list[0]",
   "id": "acace36852230751",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abseiling'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T19:39:39.466519Z",
     "start_time": "2024-09-06T19:39:39.458519Z"
    }
   },
   "cell_type": "code",
   "source": "dir(video)",
   "id": "5f7adb2b65e50b5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_avg_fps',\n",
       " '_frame_pts',\n",
       " '_handle',\n",
       " '_key_indices',\n",
       " '_num_frame',\n",
       " '_validate_indices',\n",
       " 'get_avg_fps',\n",
       " 'get_batch',\n",
       " 'get_frame_timestamp',\n",
       " 'get_key_indices',\n",
       " 'next',\n",
       " 'seek',\n",
       " 'seek_accurate',\n",
       " 'skip_frames']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T19:52:00.776991Z",
     "start_time": "2024-09-06T19:52:00.769992Z"
    }
   },
   "cell_type": "code",
   "source": "torch.tensor([2])",
   "id": "69fc67daded6a9b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8a203edecde5515f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openmmlab_v04)",
   "language": "python",
   "name": "openmmlab_v04"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
